{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vtngu\\AppData\\Local\\Temp\\ipykernel_45828\\85597204.py:10: DtypeWarning: Columns (0,2,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  classification_file = pd.read_csv(\"../dataset/preprocessed/classification.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete. Saved filtered data to ../dataset/processed/statement_cleaned.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load case_file.csv (assuming it has a column named 'serial_no')\n",
    "case_file = pd.read_csv(\"../dataset/preprocessed/case_file_cleaned.csv\", usecols=[\"serial_no\"])\n",
    "\n",
    "# Load statement.csv in chunks to handle large file efficiently\n",
    "statement_chunks = pd.read_csv(\"../dataset/preprocessed/statement.csv\", chunksize=1000000)\n",
    "\n",
    "# Convert serial_no column in case_file to a set for fast lookup\n",
    "valid_serials = set(case_file[\"serial_no\"])\n",
    "\n",
    "classification_file = pd.read_csv(\"../dataset/preprocessed/classification.csv\")\n",
    "\n",
    "# Create a new CSV file to store the cleaned data\n",
    "output_file = \"../dataset/processed/statement_cleaned.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process and filter in chunks\n",
    "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    for chunk in statement_chunks:\n",
    "        filtered_chunk = chunk[chunk[\"serial_no\"].isin(valid_serials)]\n",
    "        filtered_chunk.to_csv(f, index=False, header=f.tell()==0)  # Write header only once\n",
    "\n",
    "print(f\"Cleaning complete. Saved filtered data to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with Pseudo Mark: 112\n"
     ]
    }
   ],
   "source": [
    "statement_cleaned = pd.read_csv(output_file)\n",
    "pm_count = statement_cleaned[statement_cleaned['statement_type_cd'].str.startswith('PM')].shape[0]\n",
    "print(f\"Number of entries with Pseudo Mark: {pm_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     statement_type_cd                                     statement_text  \\\n",
      "18              PM0001                               WASHBURNS GOLD MEDAL   \n",
      "54              PM0001               SCOTTS EMULSION PPP OF PURE COD LIVE   \n",
      "67              PM0001                              WOODWARDS CRIPE WATER   \n",
      "68              PM0001                             THE DAINTY 1ST QUALITY   \n",
      "71              PM0001  NELSON MORRIS AND COMPANY BRAND EXTRA SUGAR CU...   \n",
      "...                ...                                                ...   \n",
      "1394            PM0001                                           JB SMITH   \n",
      "1401            PM0001                                         GH BARNETT   \n",
      "1403            PM0001                                                 KF   \n",
      "1419            PM0001                                                 KF   \n",
      "1422            PM0001                                            KISS-ME   \n",
      "\n",
      "      serial_no  \n",
      "18     70013253  \n",
      "54     70019225  \n",
      "67     70019979  \n",
      "68     70020157  \n",
      "71     70020175  \n",
      "...         ...  \n",
      "1394   71007902  \n",
      "1401   71007921  \n",
      "1403   71007927  \n",
      "1419   71008132  \n",
      "1422   71008144  \n",
      "\n",
      "[112 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "pm_entries = statement_cleaned[statement_cleaned['statement_type_cd'].str.startswith('PM')]\n",
    "print(pm_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load case_file_cleaned.csv (assuming it has columns named 'serial_no' and 'mark_id_char')\n",
    "case_file_cleaned = pd.read_csv(\"../dataset/preprocessed/case_file_cleaned.csv\", usecols=[\"serial_no\", \"mark_id_char\"])\n",
    "\n",
    "# Save the extracted data to mark_cleaned.csv\n",
    "case_file_cleaned.to_csv(\"../dataset/processed/mark_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns\n",
    "numerical_columns = classification_file.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Print the maximum value for each numerical column\n",
    "for column in numerical_columns:\n",
    "    max_value = classification_file[column].max()\n",
    "    print(f\"Maximum value in column '{column}': {max_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vtngu\\AppData\\Local\\Temp\\ipykernel_45828\\2158118902.py:2: DtypeWarning: Columns (0,2,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  classification_file = pd.read_csv(\"../dataset/preprocessed/classification.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered classification data saved to ../dataset/processed/classification_cleaned.csv.\n"
     ]
    }
   ],
   "source": [
    "# Load classification.csv (assuming it has a column named 'serial_no')\n",
    "classification_file = pd.read_csv(\"../dataset/preprocessed/classification.csv\")\n",
    "\n",
    "# Filter the dataframe to include only rows with serial_no in valid_serials\n",
    "classification_cleaned = classification_file[classification_file[\"serial_no\"].isin(valid_serials)]\n",
    "\n",
    "# Save the filtered data to classification_cleaned.csv\n",
    "classification_cleaned.to_csv(\"../dataset/processed/classification_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"Filtered classification data saved to ../dataset/processed/classification_cleaned.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'serial_no'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_45828\\2517364159.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m nice_class = pd.read_csv(\u001b[33m\"../dataset/processed/nice_class.csv\"\u001b[39m)  \u001b[38;5;66;03m# Contains serial_no, nice_class_code, nice_class_description\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Merge the datasets on serial_no\u001b[39;00m\n\u001b[32m      7\u001b[39m merged_df = mark_cleaned.merge(statement_cleaned, on=\u001b[33m\"serial_no\"\u001b[39m, how=\u001b[33m\"inner\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m merged_df = merged_df.merge(nice_class, on=\u001b[33m\"serial_no\"\u001b[39m, how=\u001b[33m\"inner\"\u001b[39m)\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Create the formatted column\u001b[39;00m\n\u001b[32m     11\u001b[39m merged_df[\"formatted_text\"] = merged_df.apply(\n",
      "\u001b[32mc:\\Users\\vtngu\\OneDrive\\Desktop\\GitHub\\automating_abercrombie\\venv\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10828\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10829\u001b[39m     ) -> DataFrame:\n\u001b[32m  10830\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10831\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10832\u001b[39m         return merge(\n\u001b[32m  10833\u001b[39m             self,\n\u001b[32m  10834\u001b[39m             right,\n\u001b[32m  10835\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\vtngu\\OneDrive\\Desktop\\GitHub\\automating_abercrombie\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\vtngu\\OneDrive\\Desktop\\GitHub\\automating_abercrombie\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\vtngu\\OneDrive\\Desktop\\GitHub\\automating_abercrombie\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1293\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1294\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1295\u001b[39m                         rk = cast(Hashable, rk)\n\u001b[32m   1296\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m rk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m                             right_keys.append(right._get_label_or_level_values(rk))\n\u001b[32m   1298\u001b[39m                         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1299\u001b[39m                             \u001b[38;5;66;03m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[32m   1300\u001b[39m                             right_keys.append(right.index._values)\n",
      "\u001b[32mc:\\Users\\vtngu\\OneDrive\\Desktop\\GitHub\\automating_abercrombie\\venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'serial_no'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load datasets\n",
    "mark_cleaned = pd.read_csv(\"../dataset/processed/mark_cleaned.csv\")  # Contains serial_no, mark_id_char\n",
    "statement_cleaned = pd.read_csv(\"../dataset/processed/statement_cleaned.csv\")  # Contains serial_no, statement_text\n",
    "nice_class = pd.read_csv(\"../dataset/processed/nice_class.csv\")  # Contains serial_no, nice_class_code, nice_class_description\n",
    "\n",
    "# Merge the datasets on serial_no\n",
    "merged_df = mark_cleaned.merge(statement_cleaned, on=\"serial_no\", how=\"inner\")\n",
    "merged_df = merged_df.merge(nice_class, on=\"serial_no\", how=\"inner\")\n",
    "\n",
    "# Create the formatted column\n",
    "merged_df[\"formatted_text\"] = merged_df.apply(\n",
    "    lambda row: f\"[cls] {row['mark_id_char']} [sep] {row['statement_text']} [sep] {row['nice_class_code']} [sep] {row['nice_class_description']} [cls]\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Select only the required column\n",
    "final_df = merged_df[[\"formatted_text\"]]\n",
    "\n",
    "# Save to CSV\n",
    "output_file = \"../dataset/processed/formatted_data.csv\"\n",
    "final_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Formatted data saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
